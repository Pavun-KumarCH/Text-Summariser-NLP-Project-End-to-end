{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"a45HLuJ4YIDl"},"outputs":[],"source":["#!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwG72tQHYP8V"},"outputs":[],"source":["#!pip install \"transformers[sentencepiece]\" datasets sacrebleu rouge_score py7zr -q\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGOnzkIUZZpO"},"outputs":[],"source":["#!pip install --upgrade accelerate\n","#!pip uninstall -y transformers accelerate\n","#!pip install transformers accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkcqgvNWZpMA"},"outputs":[],"source":["from transformers import pipeline, set_seed\n","from datasets import load_dataset, load_from_disk\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","from datasets import load_metric\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","from tqdm import tqdm\n","import torch\n","\n","nltk.download(\"punkt\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XMdwriFEcSAD"},"source":["### we are setup gpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZhiN0aXLbxlD"},"outputs":[],"source":["from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","device = 'cuda' if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsGHGHHlAk3l"},"outputs":[],"source":["\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n","\n","# Define the model checkpoint\n","model_ckpt = \"google/pegasus-cnn_dailymail\"\n","\n","# Instantiate the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","\n","# Instantiate the model\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKvtt-p0cHEA"},"outputs":[],"source":["#download & unzip data\n","\n","!wget = https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n","!unzip summarizer-data.zip\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"olp5aZ6c_Yhh"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XHa1CUydiTsZ"},"outputs":[],"source":["dataset_samsum = load_from_disk('samsum_dataset')\n","dataset_samsum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yS_bb5Acii-Q"},"outputs":[],"source":["split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n","\n","print(f\"Split lengths: {split_lengths}\")\n","print(f\"Features: {dataset_samsum['train'].column_names}\")\n","print(\"\\nDialogue:\")\n","\n","print(dataset_samsum['test'][1]['dialogue'])\n","\n","print(\"\\nSummary:\")\n","\n","print(dataset_samsum['test'][1]['summary'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k75p0M1njyAm"},"outputs":[],"source":["def convert_examples_to_features(example_batch):\n","  input_encoding = tokenizer(example_batch['dialogue'], max_length = 1025, truncation = True)\n","\n","  with tokenizer.as_target_tokenizer():\n","    target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True)\n","\n","    return{\n","         'input_ids' : input_encoding['input_ids'],\n","         'attention_mask' : input_encoding['attention_mask'],\n","         'labels': target_encodings['input_ids']\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_a6hAKwDmmcw"},"outputs":[],"source":["dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in7Npl3Am_XR"},"outputs":[],"source":["dataset_samsum_pt['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YF86f1FQnNAf"},"outputs":[],"source":["dataset_samsum_pt['train'][1]\n","sample = dataset_samsum_pt['train'][1]\n","\n","print(\"id:\", sample['id'])\n","print(\"dialogue:\", sample['dialogue'])\n","print(\"summary:\", sample['summary'])\n","print(\"input_ids:\", sample['input_ids'])\n","print(\"attention_mask:\", sample['attention_mask'])\n","print(\"labels:\", sample['labels'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpXYAXg_nLwR"},"outputs":[],"source":["# Training\n","\n","from transformers import DataCollatorForSeq2Seq\n","\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model = model_pegasus)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7qkWkJIn64a"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","trainer_args = TrainingArguments(\n","    output_dir = 'pegasus-samsum',\n","    num_train_epochs = 1,\n","    warmup_steps = 500,\n","    per_device_train_batch_size = 1,\n","    per_device_eval_batch_size = 1,\n","    weight_decay = 0.01,\n","    logging_steps = 10,\n","    evaluation_strategy = 'steps',\n","    eval_steps = 500, save_steps = 1e6,\n","    gradient_accumulation_steps = 16\n",")\n","\n","trainer = Trainer(model = model_pegasus,\n","                  args = trainer_args,\n","                  tokenizer = tokenizer,\n","                  data_collator = seq2seq_data_collator,\n","                  train_dataset = dataset_samsum_pt['test'],\n","                  eval_dataset = dataset_samsum_pt['validation'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hLIOZ_4IzOH8"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HxOb34I_p-Y"},"outputs":[],"source":["# Evaluation\n","\n","def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]\n","\n","\n","\n","def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device=device,\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n","                         attention_mask=inputs[\"attention_mask\"].to(device),\n","                         length_penalty=0.8, num_beams=8, max_length=128)\n","        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n","\n","        # Finally, we decode the generated texts,\n","        # replace the  token, and add the decoded texts with the references to the metric.\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                clean_up_tokenization_spaces=True)\n","               for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    #  Finally compute and return the ROUGE scores.\n","    score = metric.compute()\n","    return score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B7A8dFh_5-g"},"outputs":[],"source":["rouge_names = ['rouge1','rouge2','rougeL','rougeLsum']\n","rouge_metric = load_metric('rouge')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"As_L4TxOL9qM"},"outputs":[],"source":["score = calculate_metric_on_test_ds(\n","    dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'\n",")\n","\n","rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n","\n","pd.DataFrame(rouge_dict, index = [f'pegasus'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1-sLz84IJXk"},"outputs":[],"source":["## Save Model\n","model_pegasus.save_pretrained('pegasus-samsum-model')\n","## Save Tokenizer\n","tokenizer.save_pretrained('tokenizer')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5US1fW7fIOLq"},"outputs":[],"source":["# load\n","tokenizer = AutoTokenizer.from_pretrained('/content/tokenizer')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzZ2NdhsM09w"},"outputs":[],"source":["# prediction\n","gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n","\n","\n","\n","sample_text = dataset_samsum[\"test\"][4][\"dialogue\"]\n","\n","reference = dataset_samsum[\"test\"][4][\"summary\"]\n","\n","pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n","\n","##\n","print(\"Dialogue:\")\n","print(sample_text)\n","\n","\n","print(\"\\nReference Summary:\")\n","print(reference)\n","\n","\n","print(\"\\nModel Summary:\")\n","print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
